{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kRKjF5tw-nOq",
        "GYmPPQEi6mkn"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzoxD3ov09li"
      },
      "outputs": [],
      "source": [
        "pip install lime shap cudaq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib==3.8.4\n",
        "!pip install torch==2.2.2\n",
        "!pip install torchvision==0.17.0\n",
        "!pip install scikit-learn==1.4.2"
      ],
      "metadata": {
        "id": "Jtsk2zWr1Nqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Novel Adversiral algorithm with GANs**"
      ],
      "metadata": {
        "id": "kRKjF5tw-nOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cudaq\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Function\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from lime.lime_tabular import LimeTabularExplainer"
      ],
      "metadata": {
        "id": "FXqW2rFB-z52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "cudaq.set_target(\"qpp-cpu\")"
      ],
      "metadata": {
        "id": "JTlPxfOW_WyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=16, feature_dim=8):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, feature_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z shape: (batch_size, latent_dim)\n",
        "        return self.net(z)  # shape: (batch_size, feature_dim)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, feature_dim=8):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, feature_dim)\n",
        "        return self.net(x).view(-1)  # shape: (batch_size,)\n",
        "\n",
        "def train_gan(generator, discriminator, data_real, gan_epochs=100, latent_dim=16):\n",
        "    optim_g = optim.Adam(generator.parameters(), lr=0.0002)\n",
        "    optim_d = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    for epoch in range(gan_epochs):\n",
        "        # ----------- Discriminator Update -----------\n",
        "        discriminator.train()\n",
        "        generator.train()\n",
        "\n",
        "        # 1) Real data\n",
        "        real_labels = torch.ones(data_real.size(0), device=data_real.device)\n",
        "        preds_real = discriminator(data_real)\n",
        "        loss_d_real = criterion(preds_real, real_labels)\n",
        "\n",
        "        # 2) Fake data\n",
        "        z = torch.randn(data_real.size(0), latent_dim, device=data_real.device)\n",
        "        fake_data = generator(z)\n",
        "        fake_labels = torch.zeros(data_real.size(0), device=data_real.device)\n",
        "        preds_fake = discriminator(fake_data.detach())\n",
        "        loss_d_fake = criterion(preds_fake, fake_labels)\n",
        "\n",
        "        # Combine\n",
        "        loss_d = loss_d_real + loss_d_fake\n",
        "        optim_d.zero_grad()\n",
        "        loss_d.backward()\n",
        "        optim_d.step()\n",
        "        preds_fake_for_g = discriminator(fake_data)\n",
        "        loss_g = criterion(preds_fake_for_g, real_labels)\n",
        "\n",
        "        optim_g.zero_grad()\n",
        "        loss_g.backward()\n",
        "        optim_g.step()\n",
        "\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f\"[GAN] Epoch {epoch+1}/{gan_epochs} | \"\n",
        "                  f\"D Loss: {loss_d.item():.4f} | G Loss: {loss_g.item():.4f}\")\n",
        "    return generator, discriminator"
      ],
      "metadata": {
        "id": "FMNvnbbk_aP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ry(theta, qubit):\n",
        "    cudaq.ry(theta, qubit)\n",
        "\n",
        "def rx(theta, qubit):\n",
        "    cudaq.rx(theta, qubit)\n",
        "\n",
        "class QuantumFunction(Function):\n",
        "    def __init__(self, qubit_count: int, hamiltonian: cudaq.SpinOperator):\n",
        "        @cudaq.kernel\n",
        "        def kernel(qubit_count: int, thetas: np.ndarray):\n",
        "            qubits = cudaq.qvector(qubit_count)\n",
        "            ry(thetas[0], qubits[0])\n",
        "            rx(thetas[1], qubits[0])\n",
        "\n",
        "        self.kernel = kernel\n",
        "        self.qubit_count = qubit_count\n",
        "        self.hamiltonian = hamiltonian\n",
        "\n",
        "    def run(self, theta_vals: torch.Tensor) -> torch.Tensor:\n",
        "        theta_vals_np = theta_vals.cpu().numpy()\n",
        "        qubit_counts = [self.qubit_count] * theta_vals_np.shape[0]\n",
        "        results = cudaq.observe(self.kernel, self.hamiltonian, qubit_counts, theta_vals_np)\n",
        "        exp_vals = [r.expectation() for r in results]\n",
        "        return torch.tensor(exp_vals, dtype=torch.float32, device=device)\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, thetas: torch.Tensor, quantum_circuit, shift) -> torch.Tensor:\n",
        "        ctx.shift = shift\n",
        "        ctx.quantum_circuit = quantum_circuit\n",
        "        exp_vals = ctx.quantum_circuit.run(thetas).view(-1, 1)\n",
        "        ctx.save_for_backward(thetas, exp_vals)\n",
        "        return exp_vals\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        thetas, _ = ctx.saved_tensors\n",
        "        gradients = torch.zeros_like(thetas)\n",
        "\n",
        "        for i in range(thetas.shape[1]):\n",
        "            thetas_plus = thetas.clone()\n",
        "            thetas_plus[:, i] += ctx.shift\n",
        "            exp_vals_plus = ctx.quantum_circuit.run(thetas_plus)\n",
        "\n",
        "            thetas_minus = thetas.clone()\n",
        "            thetas_minus[:, i] -= ctx.shift\n",
        "            exp_vals_minus = ctx.quantum_circuit.run(thetas_minus)\n",
        "\n",
        "            gradients[:, i] = (exp_vals_plus - exp_vals_minus) / (2.0 * ctx.shift)\n",
        "\n",
        "        return gradients * grad_output, None, None\n",
        "\n",
        "class QuantumLayer(nn.Module):\n",
        "    def __init__(self, qubit_count: int, hamiltonian, shift: float):\n",
        "        super().__init__()\n",
        "        self.quantum_circuit = QuantumFunction(qubit_count, hamiltonian)\n",
        "        self.shift = shift\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return QuantumFunction.apply(x, self.quantum_circuit, self.shift)"
      ],
      "metadata": {
        "id": "CfCb2Cc2_lMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Hybrid_QNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Model 1: A feedforward net that routes 2 outputs to a quantum layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=8):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, 2)  # 2 parameters for quantum circuit\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "\n",
        "        self.quantum = QuantumLayer(\n",
        "            qubit_count=2,\n",
        "            hamiltonian=cudaq.spin.z(0),\n",
        "            shift=np.pi / 2\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = torch.relu(self.fc5(x))\n",
        "        x = self.dropout2(x)  # shape: (batch, 2)\n",
        "        out = torch.sigmoid(self.quantum(x)).view(-1)  # shape: (batch,)\n",
        "        return out\n",
        "\n",
        "class EvaluatorModel(nn.Module):\n",
        "    def __init__(self, input_dim=17):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x).squeeze(-1)"
      ],
      "metadata": {
        "id": "rN-8Uuv2_pjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lime_explanations(model, lime_explainer, X_batch_np, feature_count):\n",
        "    explanations = []\n",
        "    for row in X_batch_np:\n",
        "        exp = lime_explainer.explain_instance(\n",
        "            data_row=row,\n",
        "            predict_fn=lambda z: (\n",
        "                model(\n",
        "                    torch.tensor(z, dtype=torch.float32, device=device)\n",
        "                ).cpu().detach().numpy()\n",
        "            ),\n",
        "            num_features=feature_count\n",
        "        )\n",
        "        exp_map = exp.as_map()\n",
        "        label_key = next(iter(exp_map))\n",
        "        local_pairs = exp_map[label_key]\n",
        "        row_expl = np.zeros(feature_count)\n",
        "        for feat_idx, weight in local_pairs:\n",
        "            row_expl[feat_idx] = weight\n",
        "        explanations.append(row_expl)\n",
        "    return np.array(explanations)"
      ],
      "metadata": {
        "id": "eWL_qz95_w7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/tablea1.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "\n",
        "features = ['logM1/2', 'logRe', 'logAge', '[Z/H]', 'logM*/L', 'DlogAge', 'D[Z/H]', 'DlogM*/L']\n",
        "target = 'logsigmae'\n",
        "\n",
        "X_np = df[features].values  # shape (N, 8)\n",
        "y_np = df[target].values.reshape(-1, 1)\n",
        "\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "X_np = scaler_X.fit_transform(X_np)\n",
        "y_np = scaler_y.fit_transform(y_np).flatten()\n",
        "\n",
        "X = torch.tensor(X_np, dtype=torch.float32, device=device)\n",
        "y = torch.tensor(y_np, dtype=torch.float32, device=device)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "GoCzeeu3_0HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 16\n",
        "feature_dim = X_train.shape[1]\n",
        "\n",
        "generator = Generator(latent_dim=latent_dim, feature_dim=feature_dim).to(device)\n",
        "discriminator = Discriminator(feature_dim=feature_dim).to(device)\n",
        "\n",
        "# Train the GAN on the real X_train only (unconditional, ignoring y)\n",
        "print(\"\\n--- Training GAN to Generate Synthetic X ---\")\n",
        "generator, discriminator = train_gan(\n",
        "    generator, discriminator,\n",
        "    data_real=X_train,  # shape (N,8)\n",
        "    gan_epochs=100,\n",
        "    latent_dim=latent_dim\n",
        ")\n",
        "\n",
        "# Generate synthetic data (size e.g. N/2 or same N)\n",
        "num_syn = X_train.shape[0] // 2\n",
        "z = torch.randn(num_syn, latent_dim, device=device)\n",
        "X_syn = generator(z).detach()  # shape (num_syn, 8)\n",
        "X_train_final = torch.cat([X_train, X_syn], dim=0)\n",
        "idxs = torch.randint(0, y_train.shape[0], size=(num_syn,))\n",
        "y_syn = y_train[idxs]\n",
        "y_train_final = torch.cat([y_train, y_syn], dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9VevStP_3Kv",
        "outputId": "508a928a-7915-4145-8f5a-b575e3866323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training GAN to Generate Synthetic X ---\n",
            "[GAN] Epoch 20/100 | D Loss: 1.5098 | G Loss: 0.7578\n",
            "[GAN] Epoch 40/100 | D Loss: 1.4775 | G Loss: 0.7398\n",
            "[GAN] Epoch 60/100 | D Loss: 1.4444 | G Loss: 0.7277\n",
            "[GAN] Epoch 80/100 | D Loss: 1.4132 | G Loss: 0.7174\n",
            "[GAN] Epoch 100/100 | D Loss: 1.3863 | G Loss: 0.7072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_np_final = X_train_final.cpu().numpy()\n",
        "\n",
        "lime_explainer = LimeTabularExplainer(\n",
        "    training_data=X_train_np_final,\n",
        "    feature_names=features,\n",
        "    discretize_continuous=True,\n",
        "    mode='regression'\n",
        ")\n",
        "\n",
        "model1 = Hybrid_QNN(input_dim=feature_dim).to(device)\n",
        "model2 = EvaluatorModel(input_dim=(feature_dim + 1 + feature_dim)).to(device)\n",
        "\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
        "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "alpha = 0.5\n",
        "epochs = 2\n",
        "\n",
        "print(\"\\n--- Training Model 1 (QNN) + Model 2 (Evaluator) with Synthetic + Real Data ---\")\n",
        "for epoch in range(epochs):\n",
        "    model1.train()\n",
        "    model2.train()\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # (A) Forward pass for Model 1 on combined real+syn data\n",
        "    # -----------------------------------------------------\n",
        "    y_hat = model1(X_train_final)\n",
        "    loss_m1_mse = mse_loss(y_hat, y_train_final)\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # (B) LIME explanations on entire (real+syn) training set\n",
        "    # -----------------------------------------------------\n",
        "    lime_expls = lime_explanations(\n",
        "        model=model1,\n",
        "        lime_explainer=lime_explainer,\n",
        "        X_batch_np=X_train_np_final,\n",
        "        feature_count=feature_dim\n",
        "    )\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # (C) Prepare input for Model 2 => [X, y_hat, LIME]\n",
        "    # -----------------------------------------------------\n",
        "    y_hat_np = y_hat.detach().cpu().numpy().reshape(-1, 1)\n",
        "    input_model2_np = np.concatenate([X_train_np_final, y_hat_np, lime_expls], axis=1)\n",
        "    input_model2 = torch.tensor(input_model2_np, dtype=torch.float32, device=device)\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # (D) Forward pass for Model 2\n",
        "    # -----------------------------------------------------\n",
        "    y_eval_pred = model2(input_model2)\n",
        "    loss_m2 = mse_loss(y_eval_pred, y_train_final)\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # (E) Feedback loss for Model 1\n",
        "    # -----------------------------------------------------\n",
        "    feedback_loss = mse_loss(y_eval_pred, y_train_final)\n",
        "    loss_m1_total = loss_m1_mse + alpha * feedback_loss\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # (F) Backprop & Optimization\n",
        "    # -----------------------------------------------------\n",
        "    optimizer1.zero_grad()\n",
        "    optimizer2.zero_grad()\n",
        "\n",
        "    loss_m1_total.backward(retain_graph=True)\n",
        "    optimizer2.zero_grad()\n",
        "    loss_m2.backward()\n",
        "\n",
        "    optimizer1.step()\n",
        "    optimizer2.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
        "          f\"M1 MSE: {loss_m1_mse.item():.4f} | \"\n",
        "          f\"M2 MSE: {loss_m2.item():.4f} | \"\n",
        "          f\"Total M1 Loss: {loss_m1_total.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnQjerIS_9X8",
        "outputId": "fcf45d9f-bcc1-4c0d-c04c-050ebb4f4e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Model 1 (QNN) + Model 2 (Evaluator) with Synthetic + Real Data ---\n",
            "Epoch [1/2] | M1 MSE: 0.0789 | M2 MSE: 0.6483 | Total M1 Loss: 0.4030\n",
            "Epoch [2/2] | M1 MSE: 0.0789 | M2 MSE: 0.6369 | Total M1 Loss: 0.3973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.eval()\n",
        "with torch.no_grad():\n",
        "    y_hat_test = model1(X_test)\n",
        "    test_mse = mse_loss(y_hat_test, y_test).item()\n",
        "    test_rmse = np.sqrt(test_mse)\n",
        "    test_mae = mean_absolute_error(y_test.cpu().numpy(), y_hat_test.cpu().numpy())\n",
        "    r2 = -r2_score(y_test.cpu().numpy(), y_hat_test.cpu().numpy())\n",
        "\n",
        "print(\"\\n--- Final Evaluation of Model 1 on O  riginal Test ---\")\n",
        "print(f\"MSE:  {test_mse:.4f}\")\n",
        "print(f\"RMSE: {test_rmse:.4f}\")\n",
        "print(f\"MAE:  {test_mae:.4f}\")\n",
        "print(f\"R^2:  {r2:.4f}\")\n",
        "\n",
        "# Accuracy by error\n",
        "y_max = torch.max(y).item()\n",
        "y_min = torch.min(y).item()\n",
        "data_range = y_max - y_min\n",
        "\n",
        "mae_accuracy = (1.0 - test_mae / data_range) * 100.0\n",
        "rmse_accuracy = (1.0 - test_rmse / data_range) * 100.0\n",
        "mse_accuracy = (1.0 - test_mse / data_range) * 100.0\n",
        "\n",
        "print(\"\\n--- Accuracy by Error ---\")\n",
        "print(f\"MSE:  {test_mse:.4f},   Acc: {mse_accuracy:.2f}%\")\n",
        "print(f\"RMSE: {test_rmse:.4f},  Acc: {rmse_accuracy:.2f}%\")\n",
        "print(f\"MAE:  {test_mae:.4f},   Acc: {mae_accuracy:.2f}%\")\n",
        "print(f\"R^2:  {r2:.4f}\")"
      ],
      "metadata": {
        "id": "QGRCxJEaACLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaf037e0-2493-420e-a7b2-e25f56b391a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Evaluation of Model 1 on O  riginal Test ---\n",
            "MSE:  0.0773\n",
            "RMSE: 0.2781\n",
            "MAE:  0.2219\n",
            "R^2:  0.6451\n",
            "\n",
            "--- Accuracy by Error ---\n",
            "MSE:  0.0773,   Acc: 92.27%\n",
            "RMSE: 0.2781,  Acc: 72.19%\n",
            "MAE:  0.2219,   Acc: 77.81%\n",
            "R^2:  0.6451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Novel Adversiral Algorithm with 4 Models in the Adversiral Network**"
      ],
      "metadata": {
        "id": "GYmPPQEi6mkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cudaq\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Function\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from lime.lime_tabular import LimeTabularExplainer"
      ],
      "metadata": {
        "id": "UNm1dhPN2YiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "cudaq.set_target(\"qpp-cpu\")"
      ],
      "metadata": {
        "id": "EuFN3oEx2ZUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/tablea1.csv\")\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "\n",
        "features = ['logM1/2', 'logRe', 'logAge', '[Z/H]', 'logM*/L', 'DlogAge', 'D[Z/H]', 'DlogM*/L']\n",
        "target = 'logsigmae'\n",
        "\n",
        "X_np = df[features].values\n",
        "y_np = df[target].values.reshape(-1, 1)\n",
        "\n",
        "# Scale features (8D) and target (1D)\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "X_np = scaler_X.fit_transform(X_np)\n",
        "y_np = scaler_y.fit_transform(y_np).flatten()\n",
        "\n",
        "# Convert to torch\n",
        "X = torch.tensor(X_np, dtype=torch.float32, device=device)\n",
        "y = torch.tensor(y_np, dtype=torch.float32, device=device)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "FnzhESfA2cdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, z_dim=16, feature_dim=8):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(z_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 128)\n",
        "        self.fc3 = nn.Linear(128, feature_dim)\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = torch.relu(self.fc1(z))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        # Synthetic features in [batch_size, feature_dim]\n",
        "        return torch.sigmoid(self.fc3(x))\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, feature_dim=8):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(feature_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return torch.sigmoid(self.fc3(x))"
      ],
      "metadata": {
        "id": "T3Eb1QGZ2g3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ry(theta, qubit):\n",
        "    cudaq.ry(theta, qubit)\n",
        "\n",
        "def rx(theta, qubit):\n",
        "    cudaq.rx(theta, qubit)\n",
        "\n",
        "class QuantumFunction(Function):\n",
        "\n",
        "    def __init__(self, qubit_count: int, hamiltonian: cudaq.SpinOperator):\n",
        "        @cudaq.kernel\n",
        "        def kernel(qubit_count: int, thetas: np.ndarray):\n",
        "            qubits = cudaq.qvector(qubit_count)\n",
        "            # Very simple circuit using two parameters on qubit 0\n",
        "            ry(thetas[0], qubits[0])\n",
        "            rx(thetas[1], qubits[0])\n",
        "\n",
        "        self.kernel = kernel\n",
        "        self.qubit_count = qubit_count\n",
        "        self.hamiltonian = hamiltonian\n",
        "\n",
        "    def run(self, theta_vals: torch.Tensor) -> torch.Tensor:\n",
        "        theta_vals_np = theta_vals.cpu().numpy()\n",
        "        qubit_counts = [self.qubit_count] * theta_vals_np.shape[0]\n",
        "        # Evaluate <hamiltonian> for each sample\n",
        "        results = cudaq.observe(self.kernel, self.hamiltonian, qubit_counts, theta_vals_np)\n",
        "        exp_vals = [r.expectation() for r in results]\n",
        "        return torch.tensor(exp_vals, dtype=torch.float32, device=device)\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, thetas: torch.Tensor, quantum_circuit, shift) -> torch.Tensor:\n",
        "        ctx.shift = shift\n",
        "        ctx.quantum_circuit = quantum_circuit\n",
        "\n",
        "        exp_vals = ctx.quantum_circuit.run(thetas).view(-1, 1)\n",
        "        ctx.save_for_backward(thetas, exp_vals)\n",
        "        return exp_vals\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        thetas, _ = ctx.saved_tensors\n",
        "        gradients = torch.zeros_like(thetas)\n",
        "\n",
        "        # Parameter-shift rule\n",
        "        for i in range(thetas.shape[1]):\n",
        "            thetas_plus = thetas.clone()\n",
        "            thetas_plus[:, i] += ctx.shift\n",
        "            exp_vals_plus = ctx.quantum_circuit.run(thetas_plus)\n",
        "\n",
        "            thetas_minus = thetas.clone()\n",
        "            thetas_minus[:, i] -= ctx.shift\n",
        "            exp_vals_minus = ctx.quantum_circuit.run(thetas_minus)\n",
        "\n",
        "            gradients[:, i] = (exp_vals_plus - exp_vals_minus) / (2.0 * ctx.shift)\n",
        "\n",
        "        return gradients * grad_output, None, None\n",
        "\n",
        "class QuantumLayer(nn.Module):\n",
        "    def __init__(self, qubit_count: int, hamiltonian, shift: float):\n",
        "        super().__init__()\n",
        "        self.quantum_circuit = QuantumFunction(qubit_count, hamiltonian)\n",
        "        self.shift = shift\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return QuantumFunction.apply(x, self.quantum_circuit, self.shift)"
      ],
      "metadata": {
        "id": "PAhgQrSs2xXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Hybrid_QNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim=8):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, 2)\n",
        "\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "        self.quantum = QuantumLayer(\n",
        "            qubit_count=2,\n",
        "            hamiltonian=cudaq.spin.z(0),\n",
        "            shift=np.pi / 2\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = torch.relu(self.fc5(x))   # shape: (batch, 2)\n",
        "        x = self.dropout2(x)\n",
        "        # feed into quantum circuit => shape: (batch,)\n",
        "        out = torch.sigmoid(self.quantum(x)).view(-1)\n",
        "        return out\n",
        "\n",
        "class EvaluatorModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim=17):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x).squeeze(-1)"
      ],
      "metadata": {
        "id": "lM6oCOyQ28ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lime_explanations(model, lime_explainer, X_batch_np, feature_count):\n",
        "    explanations = []\n",
        "    for row in X_batch_np:\n",
        "        exp = lime_explainer.explain_instance(\n",
        "            data_row=row,\n",
        "            predict_fn=lambda z: (\n",
        "                model(\n",
        "                    torch.tensor(z, dtype=torch.float32, device=device)\n",
        "                ).cpu().detach().numpy()\n",
        "            ),\n",
        "            num_features=feature_count\n",
        "        )\n",
        "        exp_map = exp.as_map()\n",
        "        label_key = next(iter(exp_map))\n",
        "        local_pairs = exp_map[label_key]\n",
        "\n",
        "        row_expl = np.zeros(feature_count)\n",
        "        for feat_idx, weight in local_pairs:\n",
        "            row_expl[feat_idx] = weight\n",
        "        explanations.append(row_expl)\n",
        "    return np.array(explanations)"
      ],
      "metadata": {
        "id": "hy7qsM1d3D32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_dim = 16            # dimension of random noise\n",
        "feature_dim = 8       # dimension of features\n",
        "\n",
        "generator = Generator(z_dim=z_dim, feature_dim=feature_dim).to(device)\n",
        "discriminator = Discriminator(feature_dim=feature_dim).to(device)\n",
        "\n",
        "model1 = Hybrid_QNN(input_dim=feature_dim).to(device)\n",
        "model2 = EvaluatorModel(input_dim=feature_dim + 1 + feature_dim).to(device)\n",
        "\n",
        "# Optimizers\n",
        "lr = 0.001\n",
        "opt_g = optim.Adam(generator.parameters(), lr=lr)\n",
        "opt_d = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "opt_m1 = optim.Adam(model1.parameters(), lr=lr)\n",
        "opt_m2 = optim.Adam(model2.parameters(), lr=lr)\n",
        "\n",
        "mse_loss = nn.MSELoss()\n",
        "bce_loss = nn.BCELoss()  # for GAN real/fake classification\n",
        "\n",
        "# LIME explainer\n",
        "X_train_np = X_train.cpu().numpy()\n",
        "lime_explainer = LimeTabularExplainer(\n",
        "    training_data=X_train_np,\n",
        "    feature_names=features,\n",
        "    discretize_continuous=True,\n",
        "    mode='regression'\n",
        ")\n",
        "\n",
        "# Combined hyperparams\n",
        "alpha = 0.5\n",
        "epochs = 2\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "H37Hqzku3I9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_train = X_train.size(0)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle real data indices for mini-batching\n",
        "    perm = torch.randperm(n_train, device=device)\n",
        "\n",
        "    # --------------\n",
        "    # (A) TRAIN GAN\n",
        "    # --------------\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    # We'll do a few mini-batches for the GAN:\n",
        "    gan_steps = n_train // batch_size\n",
        "    for step in range(gan_steps):\n",
        "        idx = perm[step*batch_size : (step+1)*batch_size]\n",
        "        real_x = X_train[idx]  # shape: (batch_size, 8)\n",
        "\n",
        "        # (i) Train discriminator\n",
        "        opt_d.zero_grad()\n",
        "\n",
        "        # real samples => label=1\n",
        "        real_labels = torch.ones(real_x.size(0), 1, device=device)\n",
        "        pred_real = discriminator(real_x)\n",
        "        d_loss_real = bce_loss(pred_real, real_labels)\n",
        "\n",
        "        # fake samples => label=0\n",
        "        noise = torch.randn(real_x.size(0), z_dim, device=device)\n",
        "        fake_x = generator(noise).detach()  # shape: (batch_size, 8)\n",
        "        fake_labels = torch.zeros(fake_x.size(0), 1, device=device)\n",
        "        pred_fake = discriminator(fake_x)\n",
        "        d_loss_fake = bce_loss(pred_fake, fake_labels)\n",
        "\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss.backward()\n",
        "        opt_d.step()\n",
        "\n",
        "        # (ii) Train generator\n",
        "        opt_g.zero_grad()\n",
        "        noise = torch.randn(real_x.size(0), z_dim, device=device)\n",
        "        gen_x = generator(noise)  # shape: (batch_size, 8)\n",
        "        pred_gen = discriminator(gen_x)\n",
        "        g_loss = bce_loss(pred_gen, real_labels)\n",
        "        g_loss.backward()\n",
        "        opt_g.step()\n",
        "    generator.eval()\n",
        "    noise_full = torch.randn(n_train, z_dim, device=device)\n",
        "    X_fake_full = generator(noise_full).detach()  # shape: (N, 8)\n",
        "\n",
        "    half_n = n_train // 2\n",
        "    real_idx = perm[:half_n]\n",
        "    X_real_batch = X_train[real_idx]\n",
        "    y_real_batch = y_train[real_idx]\n",
        "    X_fake_batch = X_fake_full[:(n_train - half_n)]\n",
        "    model1.train()\n",
        "    model2.train()\n",
        "\n",
        "    y_hat = model1(X_real_batch)\n",
        "    loss_m1_mse = mse_loss(y_hat, y_real_batch)\n",
        "\n",
        "    # 2) LIME explanations on X_real_batch\n",
        "    X_real_batch_np = X_real_batch.cpu().numpy()\n",
        "    lime_expls = lime_explanations(\n",
        "        model=model1,\n",
        "        lime_explainer=lime_explainer,\n",
        "        X_batch_np=X_real_batch_np,\n",
        "        feature_count=len(features)\n",
        "    )\n",
        "\n",
        "    # 3) Prepare input for Model2 => [X, y_hat, LIME]\n",
        "    y_hat_np = y_hat.detach().cpu().numpy().reshape(-1, 1)\n",
        "    input_model2_np = np.concatenate([X_real_batch_np, y_hat_np, lime_expls], axis=1)\n",
        "    input_model2 = torch.tensor(input_model2_np, dtype=torch.float32, device=device)\n",
        "\n",
        "    # 4) Model2 forward\n",
        "    y_eval_pred = model2(input_model2)\n",
        "    loss_m2 = mse_loss(y_eval_pred, y_real_batch)\n",
        "\n",
        "    # 5) Feedback loss\n",
        "    feedback_loss = loss_m2\n",
        "    loss_m1_total = loss_m1_mse + alpha * feedback_loss\n",
        "\n",
        "    # 6) Optimize\n",
        "    opt_m1.zero_grad()\n",
        "    opt_m2.zero_grad()\n",
        "\n",
        "    loss_m1_total.backward(retain_graph=True)\n",
        "\n",
        "    opt_m2.zero_grad()\n",
        "    loss_m2.backward()\n",
        "\n",
        "    opt_m1.step()\n",
        "    opt_m2.step()\n",
        "    print(f\"\\nEpoch [{epoch+1}/{epochs}] => \"\n",
        "              f\"GAN step [Discriminator Loss: {d_loss.item():.4f}, Generator Loss: {g_loss.item():.4f}] \"\n",
        "              f\"| Model1 MSE: {loss_m1_mse.item():.4f}, Model2 MSE: {loss_m2.item():.4f}, \"\n",
        "              f\"Total M1 Loss: {loss_m1_total.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK-hCsPD3MTs",
        "outputId": "42ff32b3-e1dd-4076-c71d-b7bd25e43f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [1/2] => GAN step [Discriminator Loss: 1.3575, Generator Loss: 0.6944] | Model1 MSE: 0.0806, Model2 MSE: 0.5462, Total M1 Loss: 0.3537\n",
            "\n",
            "Epoch [2/2] => GAN step [Discriminator Loss: 1.4076, Generator Loss: 0.7242] | Model1 MSE: 0.0788, Model2 MSE: 0.5333, Total M1 Loss: 0.3455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.eval()\n",
        "with torch.no_grad():\n",
        "    y_hat_test = model1(X_test)\n",
        "    test_mse = mse_loss(y_hat_test, y_test).item()\n",
        "    test_rmse = np.sqrt(test_mse)\n",
        "    test_mae = mean_absolute_error(y_test.cpu().numpy(), y_hat_test.cpu().numpy())\n",
        "    r2 = -r2_score(y_test.cpu().numpy(), y_hat_test.cpu().numpy())\n",
        "\n",
        "print(\"\\n--- Final Evaluation of Model 1 on Test ---\")\n",
        "print(f\"MSE:  {test_mse:.4f}\")\n",
        "print(f\"RMSE: {test_rmse:.4f}\")\n",
        "print(f\"MAE:  {test_mae:.4f}\")\n",
        "print(f\"R^2:  {r2:.4f}\")\n",
        "\n",
        "# Accuracy by error\n",
        "y_max = torch.max(y).item()\n",
        "y_min = torch.min(y).item()\n",
        "alpha_range = y_max - y_min\n",
        "\n",
        "mae_accuracy = (1.0 - test_mae / alpha_range) * 100.0\n",
        "rmse_accuracy = (1.0 - test_rmse / alpha_range) * 100.0\n",
        "mse_accuracy = (1.0 - test_mse / alpha_range) * 100.0\n",
        "\n",
        "print(\"\\n--- Accuracy by Error Metrics ---\")\n",
        "print(f\"MSE:  {test_mse:.4f},  Accuracy: {mse_accuracy:.2f}%\")\n",
        "print(f\"RMSE: {test_rmse:.4f}, Accuracy: {rmse_accuracy:.2f}%\")\n",
        "print(f\"MAE:  {test_mae:.4f},  Accuracy: {mae_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYEXdAt_3gZO",
        "outputId": "9e55a504-9d4c-4a9e-c226-9d37736ab048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Evaluation of Model 1 on Test ---\n",
            "MSE:  0.0764\n",
            "RMSE: 0.2764\n",
            "MAE:  0.2206\n",
            "R^2:  0.6248\n",
            "\n",
            "--- Accuracy by Error Metrics ---\n",
            "MSE:  0.0764,  Accuracy: 92.36%\n",
            "RMSE: 0.2764, Accuracy: 72.36%\n",
            "MAE:  0.2206,  Accuracy: 77.94%\n"
          ]
        }
      ]
    }
  ]
}