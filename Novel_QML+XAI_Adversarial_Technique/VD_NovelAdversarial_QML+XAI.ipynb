{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEYBGfFeaodx"
      },
      "outputs": [],
      "source": [
        "pip install lime shap cudaq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQo3FDXCar-l"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib==3.8.4\n",
        "!pip install torch==2.2.2\n",
        "!pip install torchvision==0.17.0\n",
        "!pip install scikit-learn==1.4.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBDX3j6E05G3"
      },
      "source": [
        "# # Novel algo. without Quantum Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH3S_GAGdlkR",
        "outputId": "91155f2f-fd34-42da-f0e2-f4b08c20b16d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] | M1 MSE: 0.0728 | M2 MSE: 0.5843 | Total M1 Loss: 0.3650\n",
            "Epoch [2/5] | M1 MSE: 0.0665 | M2 MSE: 0.5733 | Total M1 Loss: 0.3531\n",
            "Epoch [3/5] | M1 MSE: 0.0619 | M2 MSE: 0.5617 | Total M1 Loss: 0.3428\n",
            "Epoch [4/5] | M1 MSE: 0.0616 | M2 MSE: 0.5491 | Total M1 Loss: 0.3362\n",
            "Epoch [5/5] | M1 MSE: 0.0614 | M2 MSE: 0.5363 | Total M1 Loss: 0.3296\n",
            "\n",
            "--- Final Evaluation of Model 1 on Test ---\n",
            "MSE:  0.0653\n",
            "RMSE: 0.2556\n",
            "MAE:  0.2161\n",
            "R^2:  0.3894\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "def load_data(path):\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "dataset_path = \"/content/tablea1.csv\"\n",
        "df = load_data(dataset_path)\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "\n",
        "features = ['logM1/2', 'logRe', 'logAge', '[Z/H]', 'logM*/L', 'DlogAge', 'D[Z/H]', 'DlogM*/L']\n",
        "target = 'logsigmae'\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "X = scaler_X.fit_transform(X)\n",
        "y = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "X = torch.tensor(X, dtype=torch.float32, device=device)\n",
        "y = torch.tensor(y, dtype=torch.float32, device=device)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "class Hybrid_QNN(nn.Module):\n",
        "    def __init__(self, input_dim=8):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, 2)\n",
        "        self.final_fc = nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = torch.relu(self.fc5(x))\n",
        "        out = self.final_fc(x).squeeze(-1)\n",
        "        return out\n",
        "\n",
        "class EvaluatorModel(nn.Module):\n",
        "    def __init__(self, input_dim=17):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x.squeeze(-1)\n",
        "\n",
        "def lime_explanations(model, lime_explainer, X_batch_np, feature_count):\n",
        "    explanations = []\n",
        "    for row in X_batch_np:\n",
        "        exp = lime_explainer.explain_instance(\n",
        "            data_row=row,\n",
        "            predict_fn=lambda z: (\n",
        "                model(\n",
        "                    torch.tensor(z, dtype=torch.float32, device=device)\n",
        "                ).cpu().detach().numpy()\n",
        "            ),\n",
        "            num_features=feature_count\n",
        "        )\n",
        "\n",
        "        exp_map = exp.as_map()\n",
        "        label_key = next(iter(exp_map))\n",
        "        local_pairs = exp_map[label_key]\n",
        "\n",
        "        row_expl = np.zeros(feature_count)\n",
        "        for feat_idx, weight in local_pairs:\n",
        "            row_expl[feat_idx] = weight\n",
        "\n",
        "        explanations.append(row_expl)\n",
        "    return np.array(explanations)\n",
        "\n",
        "X_train_np = X_train.cpu().numpy()\n",
        "lime_explainer = LimeTabularExplainer(\n",
        "    training_data=X_train_np,\n",
        "    feature_names=features,\n",
        "    discretize_continuous=True,\n",
        "    mode='regression'\n",
        ")\n",
        "\n",
        "model1 = Hybrid_QNN(input_dim=len(features)).to(device)\n",
        "model2 = EvaluatorModel(input_dim=(len(features) + 1 + len(features))).to(device)\n",
        "\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
        "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
        "\n",
        "mse_loss = nn.MSELoss()\n",
        "alpha = 0.5\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model1.train()\n",
        "    model2.train()\n",
        "\n",
        "    y_hat = model1(X_train)\n",
        "    loss_m1_mse = mse_loss(y_hat, y_train)\n",
        "\n",
        "    lime_expls = lime_explanations(\n",
        "        model=model1,\n",
        "        lime_explainer=lime_explainer,\n",
        "        X_batch_np=X_train_np,\n",
        "        feature_count=len(features)\n",
        "    )\n",
        "    y_hat_np = y_hat.detach().cpu().numpy().reshape(-1, 1)\n",
        "    input_model2_np = np.concatenate([X_train_np, y_hat_np, lime_expls], axis=1)\n",
        "    input_model2 = torch.tensor(input_model2_np, dtype=torch.float32, device=device)\n",
        "    y_eval_pred = model2(input_model2)\n",
        "    loss_m2 = mse_loss(y_eval_pred, y_train)\n",
        "    feedback_loss = mse_loss(y_eval_pred, y_train)\n",
        "    loss_m1_total = loss_m1_mse + alpha * feedback_loss\n",
        "    optimizer1.zero_grad()\n",
        "    optimizer2.zero_grad()\n",
        "    loss_m1_total.backward(retain_graph=True)\n",
        "    optimizer2.zero_grad()\n",
        "    loss_m2.backward()\n",
        "\n",
        "    optimizer1.step()\n",
        "    optimizer2.step()\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
        "          f\"M1 MSE: {loss_m1_mse.item():.4f} | \"\n",
        "          f\"M2 MSE: {loss_m2.item():.4f} | \"\n",
        "          f\"Total M1 Loss: {loss_m1_total.item():.4f}\")\n",
        "model1.eval()\n",
        "with torch.no_grad():\n",
        "    y_hat_test = model1(X_test)\n",
        "    test_mse = mse_loss(y_hat_test, y_test).item()\n",
        "    test_rmse = np.sqrt(test_mse)\n",
        "    test_mae = mean_absolute_error(y_test.cpu().numpy(), y_hat_test.cpu().numpy())\n",
        "    r2 = -r2_score(y_test.cpu().numpy(), y_hat_test.cpu().numpy())\n",
        "\n",
        "\n",
        "print(f\"\\n--- Final Evaluation of Model 1 on Test ---\")\n",
        "print(f\"MSE:  {test_mse:.4f}\")\n",
        "print(f\"RMSE: {test_rmse:.4f}\")\n",
        "print(f\"MAE:  {test_mae:.4f}\")\n",
        "print(f\"R^2:  {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7QYCV02HevB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c847f59b-3d14-4550-e0dd-25a6c761164f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Evaluation of Model 1 on Test ---\n",
            "MSE:  0.0653,  Accuracy by error: 93.47%\n",
            "RMSE: 0.2556, Accuracy by error: 74.44%\n",
            "MAE:  0.2161,  Accuracy by error: 78.39%\n",
            "R^2:  0.3894\n"
          ]
        }
      ],
      "source": [
        "y_max = torch.max(y).item()\n",
        "y_min = torch.min(y).item()\n",
        "alpha = y_max - y_min\n",
        "\n",
        "# Calculate accuracy by error metrics\n",
        "mae_accuracy = (1.0 - test_mae / alpha) * 100.0\n",
        "rmse_accuracy = (1.0 - test_rmse / alpha) * 100.0\n",
        "mse_accuracy = (1.0 - test_mse / alpha) * 100.0\n",
        "\n",
        "print(\"\\n--- Final Evaluation of Model 1 on Test ---\")\n",
        "print(f\"MSE:  {test_mse:.4f},  Accuracy by error: {mse_accuracy:.2f}%\")\n",
        "print(f\"RMSE: {test_rmse:.4f}, Accuracy by error: {rmse_accuracy:.2f}%\")\n",
        "print(f\"MAE:  {test_mae:.4f},  Accuracy by error: {mae_accuracy:.2f}%\")\n",
        "print(f\"R^2:  {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY0WTkwb0nYk"
      },
      "source": [
        "**The Novel Adersiral Algorithm With the Model - 1 with Quantum Layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZRZB_P-k1oJ"
      },
      "outputs": [],
      "source": [
        "import cudaq\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Function\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from lime.lime_tabular import LimeTabularExplainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dNzhsZMk3xh"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "cudaq.set_target(\"qpp-cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h52q2Haqk7Ap"
      },
      "outputs": [],
      "source": [
        "def ry(theta, qubit):\n",
        "    # Define rotation about the Y-axis\n",
        "    cudaq.ry(theta, qubit)\n",
        "\n",
        "def rx(theta, qubit):\n",
        "    # Define rotation about the X-axis\n",
        "    cudaq.rx(theta, qubit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P57eYNu2k9i9"
      },
      "outputs": [],
      "source": [
        "class QuantumFunction(Function):\n",
        "    def __init__(self, qubit_count: int, hamiltonian: cudaq.SpinOperator):\n",
        "        # Define the quantum kernel (circuit) we will run for each forward pass\n",
        "        @cudaq.kernel\n",
        "        def kernel(qubit_count: int, thetas: np.ndarray):\n",
        "            qubits = cudaq.qvector(qubit_count)\n",
        "            # For demonstration, just rotate one qubit\n",
        "            ry(thetas[0], qubits[0])\n",
        "            rx(thetas[1], qubits[0])\n",
        "\n",
        "        self.kernel = kernel\n",
        "        self.qubit_count = qubit_count\n",
        "        self.hamiltonian = hamiltonian\n",
        "\n",
        "    def run(self, theta_vals: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Evaluate the expectation value <H> for each row in theta_vals.\n",
        "        \"\"\"\n",
        "        theta_vals_np = theta_vals.cpu().numpy()\n",
        "        # 'qubit_count' is needed per sample\n",
        "        qubit_counts = [self.qubit_count] * theta_vals_np.shape[0]\n",
        "\n",
        "        # Evaluate the expectation of the given Hamiltonian\n",
        "        results = cudaq.observe(self.kernel, self.hamiltonian, qubit_counts, theta_vals_np)\n",
        "        exp_vals = [r.expectation() for r in results]\n",
        "\n",
        "        return torch.tensor(exp_vals, dtype=torch.float32, device=device)\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, thetas: torch.Tensor, quantum_circuit, shift) -> torch.Tensor:\n",
        "        # Save objects for backward pass\n",
        "        ctx.shift = shift\n",
        "        ctx.quantum_circuit = quantum_circuit\n",
        "\n",
        "        # Run the circuit to get expectation values\n",
        "        exp_vals = ctx.quantum_circuit.run(thetas).view(-1, 1)  # shape: (batch, 1)\n",
        "        ctx.save_for_backward(thetas, exp_vals)\n",
        "\n",
        "        return exp_vals\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        thetas, _ = ctx.saved_tensors\n",
        "        gradients = torch.zeros_like(thetas)\n",
        "        for i in range(thetas.shape[1]):\n",
        "            thetas_plus = thetas.clone()\n",
        "            thetas_plus[:, i] += ctx.shift\n",
        "            exp_vals_plus = ctx.quantum_circuit.run(thetas_plus)\n",
        "\n",
        "            thetas_minus = thetas.clone()\n",
        "            thetas_minus[:, i] -= ctx.shift\n",
        "            exp_vals_minus = ctx.quantum_circuit.run(thetas_minus)\n",
        "            gradients[:, i] = (exp_vals_plus - exp_vals_minus) / (2.0 * ctx.shift)\n",
        "        return gradients * grad_output, None, None\n",
        "\n",
        "class QuantumLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, qubit_count: int, hamiltonian, shift: float):\n",
        "        super().__init__()\n",
        "        self.quantum_circuit = QuantumFunction(qubit_count, hamiltonian)\n",
        "        self.shift = shift\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return QuantumFunction.apply(x, self.quantum_circuit, self.shift)\n",
        "\n",
        "\n",
        "class Hybrid_QNN(nn.Module):\n",
        "    def __init__(self, input_dim=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, 2)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "        self.quantum = QuantumLayer(\n",
        "            qubit_count=2,\n",
        "            hamiltonian=cudaq.spin.z(0),\n",
        "            shift=np.pi / 2\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # Classical feedforward part\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = torch.relu(self.fc5(x))\n",
        "        x = self.dropout2(x)\n",
        "        out = torch.sigmoid(self.quantum(x)).view(-1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class EvaluatorModel(nn.Module):\n",
        "    def __init__(self, input_dim=17):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x).squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya7sGTtElKtJ"
      },
      "outputs": [],
      "source": [
        "def lime_explanations(model, lime_explainer, X_batch_np, feature_count):\n",
        "\n",
        "    explanations = []\n",
        "    for row in X_batch_np:\n",
        "        # LIME for a single instance\n",
        "        exp = lime_explainer.explain_instance(\n",
        "            data_row=row,\n",
        "            predict_fn=lambda z: (\n",
        "                model(\n",
        "                    torch.tensor(z, dtype=torch.float32, device=device)\n",
        "                ).cpu().detach().numpy()\n",
        "            ),\n",
        "            num_features=feature_count\n",
        "        )\n",
        "        exp_map = exp.as_map()\n",
        "        label_key = next(iter(exp_map))\n",
        "        local_pairs = exp_map[label_key]\n",
        "        row_expl = np.zeros(feature_count)\n",
        "        for feat_idx, weight in local_pairs:\n",
        "            row_expl[feat_idx] = weight\n",
        "\n",
        "        explanations.append(row_expl)\n",
        "\n",
        "    return np.array(explanations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uX00fgOlOYw"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/tablea1.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Fill numeric columns with mean\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "\n",
        "features = ['logM1/2', 'logRe', 'logAge', '[Z/H]', 'logM*/L', 'DlogAge', 'D[Z/H]', 'DlogM*/L']\n",
        "target = 'logsigmae'\n",
        "\n",
        "X_np = df[features].values\n",
        "y_np = df[target].values.reshape(-1, 1)\n",
        "\n",
        "# Scale features and target\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "X_np = scaler_X.fit_transform(X_np)\n",
        "y_np = scaler_y.fit_transform(y_np).flatten()\n",
        "\n",
        "# Convert to torch\n",
        "X = torch.tensor(X_np, dtype=torch.float32, device=device)\n",
        "y = torch.tensor(y_np, dtype=torch.float32, device=device)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8PmidwFlS5B",
        "outputId": "8a2f3026-d11b-47fc-8668-0e1d0c6b1f6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/2] | M1 MSE: 0.0750 | M2 MSE: 0.2897 | Total M1 Loss: 0.2199\n",
            "Epoch [2/2] | M1 MSE: 0.0747 | M2 MSE: 0.2801 | Total M1 Loss: 0.2147\n"
          ]
        }
      ],
      "source": [
        "X_train_np = X_train.cpu().numpy()\n",
        "lime_explainer = LimeTabularExplainer(\n",
        "    training_data=X_train_np,\n",
        "    feature_names=features,\n",
        "    discretize_continuous=True,\n",
        "    mode='regression'\n",
        ")\n",
        "\n",
        "# Instantiate Model 1 (Hybrid QNN) and Model 2 (Evaluator)\n",
        "model1 = Hybrid_QNN(input_dim=len(features)).to(device)\n",
        "model2 = EvaluatorModel(input_dim=(len(features) + 1 + len(features))).to(device)\n",
        "\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
        "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "# Weight for feedback term\n",
        "alpha = 0.5\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    model1.train()\n",
        "    model2.train()\n",
        "    y_hat = model1(X_train)\n",
        "    loss_m1_mse = mse_loss(y_hat, y_train)\n",
        "    lime_expls = lime_explanations(\n",
        "        model=model1,\n",
        "        lime_explainer=lime_explainer,\n",
        "        X_batch_np=X_train_np,\n",
        "        feature_count=len(features)\n",
        "    )\n",
        "    y_hat_np = y_hat.detach().cpu().numpy().reshape(-1, 1)\n",
        "    input_model2_np = np.concatenate([X_train_np, y_hat_np, lime_expls], axis=1)\n",
        "    input_model2 = torch.tensor(input_model2_np, dtype=torch.float32, device=device)\n",
        "    y_eval_pred = model2(input_model2)\n",
        "    loss_m2 = mse_loss(y_eval_pred, y_train)\n",
        "    feedback_loss = mse_loss(y_eval_pred, y_train)\n",
        "    loss_m1_total = loss_m1_mse + alpha * feedback_loss\n",
        "    optimizer1.zero_grad()\n",
        "    optimizer2.zero_grad()\n",
        "    loss_m1_total.backward(retain_graph=True)\n",
        "    optimizer2.zero_grad()\n",
        "    loss_m2.backward()\n",
        "    optimizer1.step()\n",
        "    optimizer2.step()\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
        "          f\"M1 MSE: {loss_m1_mse.item():.4f} | \"\n",
        "          f\"M2 MSE: {loss_m2.item():.4f} | \"\n",
        "          f\"Total M1 Loss: {loss_m1_total.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2by9bjeAlV-4",
        "outputId": "cca7892d-f93e-43a0-eefd-95e07f737c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Evaluation of Model 1 on Test ---\n",
            "MSE:  0.0751\n",
            "RMSE: 0.2740\n",
            "MAE:  0.2189\n",
            "R^2:  0.5978\n"
          ]
        }
      ],
      "source": [
        "model1.eval()\n",
        "with torch.no_grad():\n",
        "    y_hat_test = model1(X_test)\n",
        "    test_mse = mse_loss(y_hat_test, y_test).item()\n",
        "    test_rmse = np.sqrt(test_mse)\n",
        "    test_mae = mean_absolute_error(y_test.cpu().numpy(), y_hat_test.cpu().numpy())\n",
        "    r2 = r2_score(y_test.cpu().numpy(), y_hat_test.cpu().numpy())\n",
        "\n",
        "print(\"\\n--- Final Evaluation of Model 1 on Test ---\")\n",
        "print(f\"MSE:  {test_mse:.4f}\")\n",
        "print(f\"RMSE: {test_rmse:.4f}\")\n",
        "print(f\"MAE:  {test_mae:.4f}\")\n",
        "print(f\"R^2:  {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2qzcntA1kc8",
        "outputId": "0852ed4e-3862-42ae-81db-ffb0b8449157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Evaluation of Model 1 on Test ---\n",
            "MSE:  0.0751,  Accuracy by error: 92.49%\n",
            "RMSE: 0.2740, Accuracy by error: 72.60%\n",
            "MAE:  0.2189,  Accuracy by error: 78.11%\n",
            "R^2:  0.5978\n"
          ]
        }
      ],
      "source": [
        "y_max = torch.max(y).item()\n",
        "y_min = torch.min(y).item()\n",
        "alpha = y_max - y_min\n",
        "\n",
        "# Calculate accuracy by error metrics\n",
        "mae_accuracy = (1.0 - test_mae / alpha) * 100.0\n",
        "rmse_accuracy = (1.0 - test_rmse / alpha) * 100.0\n",
        "mse_accuracy = (1.0 - test_mse / alpha) * 100.0\n",
        "\n",
        "print(\"\\n--- Final Evaluation of Model 1 on Test ---\")\n",
        "print(f\"MSE:  {test_mse:.4f},  Accuracy by error: {mse_accuracy:.2f}%\")\n",
        "print(f\"RMSE: {test_rmse:.4f}, Accuracy by error: {rmse_accuracy:.2f}%\")\n",
        "print(f\"MAE:  {test_mae:.4f},  Accuracy by error: {mae_accuracy:.2f}%\")\n",
        "print(f\"R^2:  {-r2:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}